<?xml version="1.0"?>
<!-- TODO MiconCodeReview: From external project user viewpoint, it would be more appropriate to call this a 'main' instead of 'chain' workflow. -->
<!-- TODO MiconCodeReview: E.g.: the main workflow in icm-iis-collapsers is called just 'main'. -->
<!-- TODO MiconCodeReview: Also, 'citationmatching_' prefix is unnecessary (it's encoded in the path eu.dnetlib.iis.citationmatching). -->
<!-- TODO MiconCodeReview: In case of renaming, the package should be renamed accordingly ('chain' -> 'main'). -->
<workflow-app xmlns="uri:oozie:workflow:0.4" name="citationmatching_chain">

	<parameters>
        <property>
            <!-- TODO MiconCodeReview: Property name could be more meaningful, like 'inputMetadata'. -->
            <name>input</name>
            <description>input directory holding eu.dnetlib.iis.citationmatching.schemas.DocumentMetadata avro datastore</description>
        </property>
        <property>
            <!-- TODO MiconCodeReview: Property name could be more meaningful, like 'outputCitations'. -->
            <name>output</name>
            <description>output directory holding eu.dnetlib.iis.citationmatching.schemas.Citation avro datastore</description>
        </property>
        <property>
            <name>reduceTasks</name>
            <value>30</value>
        </property>
        <property>
            <name>remove_sideproducts</name>
            <value>true</value>
        </property>
    </parameters>

    <global>
        <job-tracker>${jobTracker}</job-tracker>
        <name-node>${nameNode}</name-node>
        <configuration>
            <property>
                <name>io.serializations</name>
                <value>org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization,org.apache.avro.hadoop.io.AvroSerialization
                </value>
            </property>
            <property>
                <name>rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB
                </name>
                <value>org.apache.hadoop.ipc.ProtobufRpcEngine</value>
            </property>
            <!-- ## This is required for new MapReduce API usage -->
            <property>
                <name>mapred.mapper.new-api</name>
                <value>true</value>
            </property>
            <property>
                <name>mapred.reducer.new-api</name>
                <value>true</value>
            </property>
        </configuration>
    </global>

    <start to="generate-schema" />

	<action name="generate-schema">
	    <java>
	        <main-class>eu.dnetlib.iis.core.javamapreduce.hack.AvroSchemaGenerator</main-class>
	        <arg>eu.dnetlib.iis.citationmatching.schemas.DocumentMetadata</arg>
	        <arg>eu.dnetlib.iis.citationmatching.schemas.PartialCitation</arg>
	        <arg>eu.dnetlib.iis.citationmatching.schemas.Citation</arg>
	        <capture-output />
	    </java>
	    <ok to="avro_to_protobuf_forking" />
	    <error to="fail" />
	</action>

    <fork name="avro_to_protobuf_forking">
        <path start="avro_to_protobuf_document"/>
        <path start="avro_to_protobuf_reference"/>
    </fork>

    <action name="avro_to_protobuf_document">
        <map-reduce>
            <!-- The data generated by this node is deleted in this section -->
            <prepare>
                <delete path="${nameNode}${workingDir}/destination_entities" />
            </prepare>
            <configuration>
                <property>
                    <name>mapreduce.inputformat.class</name>
                    <value>org.apache.avro.mapreduce.AvroKeyInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat</value>
                </property>
                <property>
                    <name>mapred.mapoutput.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapred.mapoutput.value.class</name>
                    <value>org.apache.hadoop.io.BytesWritable</value>
                </property>
                <property>
                    <name>mapred.reduce.tasks</name>
                    <value>0</value>
                </property>
                <property>
                    <name>mapred.output.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapred.output.value.class</name>
                    <value>org.apache.hadoop.io.BytesWritable</value>
                </property>
                <property>
                    <name>mapreduce.map.class</name>
                    <value>eu.dnetlib.iis.workflows.citationmatching.converter.AvroDocumentMetadataToProtoBufMatchableEntityMapper</value>
                </property>
                <property>
                    <name>avro.schema.input.key</name>
                    <value>${wf:actionData('generate-schema')['eu.dnetlib.iis.citationmatching.schemas.DocumentMetadata']}</value>
                </property>
                <property>
                    <name>mapred.input.dir</name>
                    <value>${input}</value>
                </property>
                <property>
                    <name>mapred.output.dir</name>
                    <value>${workingDir}/destination_entities</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="avro_to_protobuf_joining" />
        <error to="fail" />
    </action>

    <action name="avro_to_protobuf_reference">
        <map-reduce>
            <!-- The data generated by this node is deleted in this section -->
            <prepare>
                <delete path="${nameNode}${workingDir}/source_entities" />
            </prepare>
            <configuration>
                <property>
                    <name>mapreduce.inputformat.class</name>
                    <value>org.apache.avro.mapreduce.AvroKeyInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat</value>
                </property>

                <property>
                    <name>mapred.mapoutput.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapred.mapoutput.value.class</name>
                    <value>org.apache.hadoop.io.BytesWritable</value>
                </property>
                <property>
                    <name>mapred.reduce.tasks</name>
                    <value>0</value>
                </property>

                <property>
                    <name>mapred.output.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapred.output.value.class</name>
                    <value>org.apache.hadoop.io.BytesWritable</value>
                </property>
                <property>
                    <name>mapreduce.map.class</name>
                    <value>eu.dnetlib.iis.workflows.citationmatching.converter.AvroDocumentMetadataReferencesToProtoBufMatchableEntityMapper</value>
                </property>
                <property>
                    <name>avro.schema.input.key</name>
                    <value>${wf:actionData('generate-schema')['eu.dnetlib.iis.citationmatching.schemas.DocumentMetadata']}</value>
                </property>
                <property>
                    <name>mapred.input.dir</name>
                    <value>${input}</value>
                </property>
                <property>
                    <name>mapred.output.dir</name>
                    <value>${workingDir}/source_entities</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="avro_to_protobuf_joining" />
        <error to="fail" />
    </action>

    <!-- TODO MiconCodeReview: Action name could be more meaningful, like 'coansys-citationmatching'. -->
    <join name="avro_to_protobuf_joining" to="coansys"/>

    <action name="coansys">
        <sub-workflow>
            <app-path>${wf:appPath()}/coansys</app-path>
            <configuration>
                <property>
                    <name>jobTracker</name>
                    <value>${jobTracker}</value>
                </property>
                <property>
                    <name>nameNode</name>
                    <value>${nameNode}</value>
                </property>
                <property>
                    <name>queueName</name>
                    <value>${queueName}</value>
                </property>
                <property>
                    <name>reduceTasks</name>
                    <value>${reduceTasks}</value>
                </property>
                <property>
                    <name>jobShuffleInputBufferPercent</name>
                    <value>0.30</value>
                </property>
                <property>
                    <name>mapredChildJavaOpts</name>
                    <value>-Xmx4000m</value>
                </property>
                <property>
                    <name>workingDirectory</name>
                    <value>${nameNode}${workingDir}/coansys</value>
                </property>
                <property>
                    <name>sourceEntities</name>
                    <value>${nameNode}${workingDir}/source_entities</value>
                </property>
                <property>
                    <name>destinationEntities</name>
                    <value>${nameNode}${workingDir}/destination_entities</value>
                </property>
                <property>
                    <name>output</name>
                    <!-- TODO MiconCodeReview: In case of changing action name, this should become '${nameNode}${workingDir}/coansys-citationmatching/output'. -->
                    <value>${nameNode}${workingDir}/coansys/matching</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="protobuf_to_avro_and_reference_attacher"/>
        <error to="fail"/>
    </action>

    <!-- TODO MiconCodeReview: Action name could be more meaningful, like 'protobuf_to_avro_and_raw_text_generator' or just 'postprocessing'. -->
    <action name="protobuf_to_avro_and_reference_attacher">
        <map-reduce>
            <prepare>
                <delete path="${nameNode}${output}" />
            </prepare>
            <configuration>
                <!-- TODO MiconCodeReview: There could be a comment that would make easier to understand the workflow, like: -->
                <!-- TODO MiconCodeReview: ## Multiple intermediate mappers (instead of subworkflows) -->
                <property>
                    <name>mapreduce.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.DelegatingInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.map.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.DelegatingMapper</value>
                </property>
                <!-- TODO MiconCodeReview: ### Intermediate mappers input directories, classes & output directories -->
                <property>
                    <name>mapred.input.dir.mappers</name>
                    <value>
                        ${nameNode}${workingDir}/coansys/matching;eu.dnetlib.iis.workflows.citationmatching.converter.MatchingToAvroPartialCitationMapper
                    </value>
                </property>
                <property>
                    <name>mapred.input.dir.formats</name>
                    <value>
                        ${nameNode}${workingDir}/coansys/matching;org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat
                    </value>
                </property>
                <!-- TODO MiconCodeReview: ### Intermediate mappers schemas -->
                <property>
                    <name>mapreduce.outputformat.class</name>
                    <value>org.apache.avro.mapreduce.AvroKeyOutputFormat</value>
                </property>
                <property>
                    <name>mapred.mapoutput.key.class</name>
                    <value>org.apache.avro.mapred.AvroKey</value>
                </property>
                <property>
                    <name>mapred.mapoutput.value.class</name>
                    <value>org.apache.avro.mapred.AvroValue</value>
                </property>
                <property>
                    <name>mapred.output.key.class</name>
                    <value>org.apache.avro.mapred.AvroKey</value>
                </property>
                <property>
                    <name>mapred.output.value.class</name>
                    <value>org.apache.hadoop.io.NullWritable</value>
                </property>
                <property>
                    <name>mapred.output.key.comparator.class</name>
                    <value>org.apache.avro.hadoop.io.AvroKeyComparator</value>
                </property>
                <property>
                    <name>mapred.output.value.groupfn.class</name>
                    <value>org.apache.avro.hadoop.io.AvroKeyComparator</value>
                </property>
                <property>
                    <name>mapred.reduce.tasks</name>
                    <value>${reduceTasks}</value>
                </property>
                <!-- TODO MiconCodeReview: 'Input & final output' section could be placed at the beginning of the workflow. -->
                <!-- TODO MiconCodeReview: There could be a comment that would make the workflow easier to understand, like: -->
                <!-- TODO MiconCodeReview: ## Input & final output -->
                <!-- ### Schema of the input -->
                <property>
                    <name>avro.schema.input.key</name>
                    <value>${wf:actionData('generate-schema')['eu.dnetlib.iis.citationmatching.schemas.DocumentMetadata']}</value>
                </property>
                <!-- ### Schemas of the data produced by the mapper -->
                <property>
                    <name>avro.serialization.key.reader.schema</name>
                    <value>"string"</value>
                </property>
                <property>
                    <name>avro.serialization.key.writer.schema</name>
                    <value>"string"</value>
                </property>
                <property>
                    <name>avro.serialization.value.reader.schema</name>
                    <value>${wf:actionData('generate-schema')['eu.dnetlib.iis.citationmatching.schemas.PartialCitation']}</value>
                </property>
                <property>
                    <name>avro.serialization.value.writer.schema</name>
                    <value>${wf:actionData('generate-schema')['eu.dnetlib.iis.citationmatching.schemas.PartialCitation']}</value>
                </property>
                <!-- ### Schema of the data produced by the reducer -->
                <property>
                    <name>avro.schema.output.key</name>
                    <value>${wf:actionData('generate-schema')['eu.dnetlib.iis.citationmatching.schemas.Citation']}</value>
                </property>
                <!-- ## Reducer class -->
                <property>
                    <name>mapreduce.reduce.class</name>
                    <value>eu.dnetlib.iis.workflows.citationmatching.PartialCitationMerger</value>
                </property>
                <!-- TODO MiconCodeReview: ### Output directory -->
                <property>
                    <name>mapred.output.dir</name>
                    <value>${output}</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="finalize" />
        <error to="fail" />
    </action>

	<decision name="finalize">
		<switch>
			<case to="remove_sideproducts">${remove_sideproducts eq "true"}</case>
			<default to="end" />
		</switch>
	</decision>
	
	<action name="remove_sideproducts">
		<fs>
			<delete path="${nameNode}${workingDir}/coansys" />
		</fs>
		<ok to="end" />
		<error to="fail" />
	</action>

    <kill name="fail">
        <message>Unfortunately, the process failed -- error message:
            [${wf:errorMessage(wf:lastErrorNode())}]
        </message>
    </kill>
    <end name="end" />
</workflow-app>
