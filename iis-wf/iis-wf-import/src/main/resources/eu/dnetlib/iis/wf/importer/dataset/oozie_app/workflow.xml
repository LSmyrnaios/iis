<workflow-app xmlns="uri:oozie:workflow:0.4" name="importer_dataset">

    <parameters>
        <property>
            <name>mdstore_facade_factory_classname</name>
            <value>eu.dnetlib.iis.wf.importer.facade.WebServiceMDStoreFacadeFactory</value>
            <description>ServiceFacadeFactory implementation class name producing eu.dnetlib.iis.wf.importer.facade.MDStoreFacade</description>
        </property>
        <property>
            <name>mdstore_service_location</name>
            <value>$UNDEFINED$</value>
            <description>MDStore service (not WSDL) location URL</description>
        </property>
        <property>
            <name>mdstore_ids_csv</name>
            <value>$UNDEFINED$</value>
            <description>MDStores identifiers</description>
        </property>
        <property>
            <name>mdstore_record_maxlength</name>
            <value>500000</value>
            <description>maximum allowed length of mdstore record</description>
        </property>
        <property>
            <name>input_mdstore_resultset_pagesize</name>
            <value>100</value>
            <description>ResultSet single page size</description>
        </property>
        <!-- output -->
        <property>
            <name>output_root</name>
            <description>output root location</description>
        </property>
        <property>
            <name>output_name_dataset</name>
            <value>meta</value>
            <description>dataset metadata output directory</description>
        </property>
        <property>
            <name>output_name_dataset_text</name>
            <value>text</value>
            <description>dataset to mdstore mapping output directory</description>
        </property>
        <property>
            <name>dnet_service_client_read_timeout</name>
            <value>60000</value>
            <description>DNet service client reading timeout (expressed in milliseconds)</description>
        </property>
        <property>
            <name>dnet_service_client_connection_timeout</name>
            <value>60000</value>
            <description>DNet service client connection timeout (expressed in milliseconds)</description>
        </property>
        <property>
            <name>resultset_client_read_timeout</name>
            <value>60000</value>
            <description>result set client reading timeout (expressed in milliseconds)</description>
        </property>
        <property>
            <name>resultset_client_connection_timeout</name>
            <value>60000</value>
            <description>result set client connection timeout (expressed in milliseconds)</description>
        </property>
        <property>
            <name>output_report_root_path</name>
            <description>base directory for storing reports</description>
        </property>
        <property>
            <name>output_report_relative_path</name>
            <value>import_dataset</value>
            <description>directory for storing report (relative to output_report_root_path)</description>
        </property>
    </parameters>

    <global>
        <job-tracker>${jobTracker}</job-tracker>
        <name-node>${nameNode}</name-node>
        <configuration>
            <property>
                <name>mapreduce.job.queuename</name>
                <value>${importerQueueName}</value>
            </property>
            <property>
                <name>oozie.launcher.mapred.job.queue.name</name>
                <value>${oozieLauncherQueueName}</value>
            </property>
        </configuration>
    </global>


    <start to="generate-schema" />

    <action name="generate-schema">
        <java>
            <main-class>eu.dnetlib.iis.common.javamapreduce.hack.AvroSchemaGenerator</main-class>
            <arg>eu.dnetlib.iis.common.schemas.Identifier</arg>
            <arg>eu.dnetlib.iis.importer.schemas.DataSetReference</arg>
            <arg>eu.dnetlib.iis.metadataextraction.schemas.DocumentText</arg>
            <capture-output />
        </java>
        <ok to="mdstoreid-importer" />
        <error to="fail" />
    </action>

    <action name="mdstoreid-importer">
        <java>
            <!-- The data generated by this node is deleted in this section -->
            <prepare>
                <delete path="${nameNode}${workingDir}/identifier" />
            </prepare>
            <main-class>eu.dnetlib.iis.common.java.ProcessWrapper</main-class>
            <arg>eu.dnetlib.iis.wf.importer.dataset.MDStoreIdentifierDatastoreBuilder</arg>
            <arg>-Pimport.mdstore.ids.csv=${mdstore_ids_csv}</arg>
            <arg>-Oidentifier=${workingDir}/identifier</arg>
        </java>
        <ok to="dataset-importer" />
        <error to="fail" />
    </action>


    <action name="dataset-importer">
        <map-reduce>
            <!-- The data generated by this node is deleted in this section -->
            <prepare>
                <delete path="${nameNode}${output_root}" />
            </prepare>
            <configuration>
                <property>
                    <name>import.mdstore.service.location</name>
                    <value>${mdstore_service_location}</value>
                </property>
                <property>
                    <name>import.mdstore.record.maxlength</name>
                    <value>${mdstore_record_maxlength}</value>
                </property>
                <property>
                    <name>import.resultset.pagesize</name>
                    <value>${input_mdstore_resultset_pagesize}</value>
                </property>
                <property>
                    <name>dnet.service.client.read.timeout</name>
                    <value>${dnet_service_client_read_timeout}</value>
                </property>
                <property>
                    <name>dnet.service.client.connection.timeout</name>
                    <value>${dnet_service_client_connection_timeout}</value>
                </property>     
                <property>
                    <name>import.resultset.client.read.timeout</name>
                    <value>${resultset_client_read_timeout}</value>
                </property>
                <property>
                    <name>import.resultset.client.connection.timeout</name>
                    <value>${resultset_client_connection_timeout}</value>
                </property>
                <property>
                    <name>import.facade.factory.classname</name>
                    <value>${mdstore_facade_factory_classname}</value>
                </property>
                <!-- disabling speculative run to be less invasive to MDStore -->
                <property>
                    <name>mapreduce.map.speculative</name>
                    <value>false</value>
                </property>
                <property>
                    <name>mapreduce.task.timeout</name>
                    <value>14400000</value>
                </property>
                <property>
                    <name>mapreduce.job.inputformat.class</name>
                    <value>org.apache.avro.mapreduce.AvroKeyInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.map.output.key.class</name>
                    <value>org.apache.avro.mapred.AvroKey</value>
                </property>
                <property>
                    <name>mapreduce.map.output.value.class</name>
                    <value>org.apache.avro.mapred.AvroValue</value>
                </property>
                <property>
                    <name>mapreduce.job.output.key.class</name>
                    <value>org.apache.avro.mapred.AvroKey</value>
                </property>
                <property>
                    <name>mapreduce.job.output.value.class</name>
                    <value>org.apache.avro.mapred.AvroValue</value>
                </property>
                <property>
                    <name>mapreduce.job.output.key.comparator.class</name>
                    <value>org.apache.avro.hadoop.io.AvroKeyComparator</value>
                </property>
                <property>
                    <name>io.serializations</name>
                    <value>org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization,org.apache.avro.hadoop.io.AvroSerialization</value>
                </property>
                <property>
                    <name>mapreduce.job.output.group.comparator.class</name>
                    <value>org.apache.avro.hadoop.io.AvroKeyComparator</value>
                </property>
                <property>
                    <name>rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB</name>
                    <value>org.apache.hadoop.ipc.ProtobufRpcEngine</value>
                </property>
                <!-- This is required for new api usage -->
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>

                <!-- Standard stuff for our framework -->

                <property>
                    <name>avro.schema.input.key</name>
                    <value>${wf:actionData('generate-schema')['eu.dnetlib.iis.common.schemas.Identifier']}</value>
                </property>

                <property>
                    <name>avro.mapreduce.multipleoutputs</name>
                    <value>${output_name_dataset} ${output_name_dataset_text}</value>
                </property>
                <!-- ## Output classes for all output ports -->
                <property>
                    <name>avro.mapreduce.multipleoutputs.namedOutput.${output_name_dataset}.format</name>
                    <value>org.apache.avro.mapreduce.AvroKeyOutputFormat</value>
                </property>
                <property>
                    <name>avro.mapreduce.multipleoutputs.namedOutput.${output_name_dataset_text}.format</name>
                    <value>org.apache.avro.mapreduce.AvroKeyOutputFormat</value>
                </property>
                <!-- ### Schema of multiple output ports. -->
                <property>
                    <name>avro.mapreduce.multipleoutputs.namedOutput.${output_name_dataset}.keyschema</name>
                    <value>${wf:actionData('generate-schema')['eu.dnetlib.iis.importer.schemas.DataSetReference']}</value>
                </property>
                <property>
                    <name>avro.mapreduce.multipleoutputs.namedOutput.${output_name_dataset_text}.keyschema</name>
                    <value>${wf:actionData('generate-schema')['eu.dnetlib.iis.metadataextraction.schemas.DocumentText']}</value>
                </property>

                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>${workingDir}/identifier</value>
                </property>
                <property>
                    <name>mapreduce.output.fileoutputformat.outputdir</name>
                    <value>${output_root}</value>
                </property>
                <property>
                    <name>output.dataset</name>
                    <value>${output_name_dataset}</value>
                </property>
                <property>
                    <name>output.dataset_text</name>
                    <value>${output_name_dataset_text}</value>
                </property>
                <property>
                    <name>mapreduce.job.map.class</name>
                    <value>eu.dnetlib.iis.wf.importer.dataset.DatasetImporterMapper</value>
                </property>
                <property>
                    <name>mapreduce.job.reduces</name>
                    <value>0</value>
                </property>
                <property>
                    <name>oozie.action.external.stats.write</name>
                    <value>true</value>
                </property>
                <property>
                    <name>avro.mapreduce.multipleoutputs.counters</name>
                    <value>true</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="report" />
        <error to="fail" />
    </action>

    <action name="report">
        <java>
            <main-class>eu.dnetlib.iis.common.java.ProcessWrapper</main-class>
            <arg>eu.dnetlib.iis.common.report.ReportGenerator</arg>
            <arg>-Preport.import.dataset.dataset=${hadoop:counters('dataset-importer')['eu.dnetlib.iis.common.javamapreduce.hack.AvroMultipleOutputs'][concat(output_name_dataset,'/part')]}</arg>
            <arg>-Preport.import.dataset.mdrecord=${hadoop:counters('dataset-importer')['eu.dnetlib.iis.common.javamapreduce.hack.AvroMultipleOutputs'][concat(output_name_dataset_text,'/part')]}</arg>
            <arg>-Preport.import.dataset.invalid.sizeExceeded=${hadoop:counters('dataset-importer')['eu.dnetlib.iis.wf.importer.dataset.DatasetImporterMapper$InvalidRecordCounters']['SIZE_EXCEEDED']}</arg>
            <arg>-Oreport=${output_report_root_path}/${output_report_relative_path}</arg>
        </java>
        <ok to="end" />
        <error to="fail" />
    </action>

    <kill name="fail">
        <message>Unfortunately, the process failed -- error message: [${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
    <end name="end" />
</workflow-app>
